{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372fd343",
   "metadata": {},
   "source": [
    "# Copyright (c) 2025 YI-AN YEH\n",
    "# This project is licensed under the MIT License - see the LICENSE file for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bba120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "Device name: NVIDIA GeForce RTX 2070 SUPER\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 檢查是否有可用的 CUDA 設備 (NVIDIA GPU)\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available:\n",
    "    # 獲取 GPU 設備\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3b5f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c3ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image # Pillow 函式庫，用於讀取圖片\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 全域設定參數 ---\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 15\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6973495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練驗證集資料量: 16510\n"
     ]
    }
   ],
   "source": [
    "# 載入我們在 Notebook 01 中切分好的資料集\n",
    "train_val_df = pd.read_csv('train_val_set.csv')\n",
    "test_df = pd.read_csv('test_set.csv')\n",
    "\n",
    "# 載入標籤與索引的對應關係\n",
    "with open('label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "print(f\"訓練驗證集資料量: {len(train_val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a242bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定義資料增強與轉換\n",
    "# 我們建立兩種轉換流程：一種用於訓練 (包含隨機增強)，一種用於驗證 (只有標準化)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(IMG_SIZE), # 隨機裁切並縮放回 IMG_SIZE\n",
    "        transforms.RandomHorizontalFlip(),      # 隨機水平翻轉\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # 隨機調整顏色\n",
    "        transforms.ToTensor(),                  # 將圖片轉換為 PyTorch Tensor，並將像素值縮放到 [0, 1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 用 ImageNet 的平均值和標準差進行標準化\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),                 # 先縮放到 256\n",
    "        transforms.CenterCrop(IMG_SIZE),        # 從中心裁切出 IMG_SIZE 的區域\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 建立自定義的 Dataset 類別\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # 回傳資料集的總長度\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根據索引 (idx) 獲取一筆資料\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        label = self.df.iloc[idx]['label_idx']\n",
    "        \n",
    "        # 使用 Pillow 讀取圖片\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 套用指定的轉換/增強\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2268a36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        # 繼承 nn.Module 的初始化\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        # --- 在 __init__ 中，像樂高積木一樣，先定義好模型會用到的所有 \"層\" ---\n",
    "        \n",
    "        # 特徵提取層 (Convolutional Layers)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same')\n",
    "        \n",
    "        # 分類層 (Fully Connected Layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # 這裡的 in_features 需要手動計算\n",
    "        # 原始圖片 224x224，經過 3 次 MaxPool (每次尺寸減半) -> 224 / 2 / 2 / 2 = 28\n",
    "        # 所以展平前的 Tensor 維度是 (batch_size, 128, 28, 28)\n",
    "        # 展平後變成 (batch_size, 128 * 28 * 28)\n",
    "        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=512)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- 在 forward 函式中，定義資料 \"向前傳播\" 的順序 ---\n",
    "        # 也就是說，資料要依序通過我們剛剛定義的那些層\n",
    "        \n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Classification Head\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # 註：輸出的原始分數 (logits) 直接給損失函數即可，\n",
    "                         # PyTorch 的 CrossEntropyLoss 會自動幫我們做 Softmax\n",
    "        return x\n",
    "\n",
    "# 建立模型實例\n",
    "baseline_model = BaselineModel(NUM_CLASSES)\n",
    "# 將模型的所有參數和緩衝區移動到我們之前設定的 device (GPU) 上\n",
    "baseline_model.to(device)\n",
    "\n",
    "# 印出模型架構，確認一下\n",
    "print(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf90364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # 引入 tqdm 來顯示一個漂亮的進度條\n",
    "\n",
    "# 訓練一個 Epoch 的函數\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # 將模型設置為訓練模式 (這對 Dropout、BatchNorm 等層很重要)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # 使用 tqdm 包裝 dataloader 來顯示進度條\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 1. 將資料移動到 GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 2. 清零梯度 (非常重要的一步)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 3. 前向傳播\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 4. 計算損失\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 5. 反向傳播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 6. 更新權重\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累計損失和正確預測數\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "\n",
    "# 驗證一個 Epoch 的函數\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()  # 將模型設置為評估模式\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # 在評估模式下，我們不需要計算梯度，可以節省計算資源\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            # 1. 將資料移動到 GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 2. 前向傳播\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 3. 計算損失\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 累計損失和正確預測數\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 檢查 PyTorch 能否偵測到 CUDA (GPU)\n",
    "print(f\"CUDA 是否可用: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 2. 獲取 GPU 的名稱\n",
    "    print(f\"目前 GPU 名稱: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # 3. 這是 \"正確\" 的 device 定義方式\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"警告: PyTorch 找不到 CUDA 設備，將使用 CPU。\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 4. 打印出您目前 \"真正\" 在使用的設備\n",
    "print(f\"目前程式將使用的設備: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85269e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "\n",
    "# --- Heads-up: 這段程式碼會運行很長時間！ ---\n",
    "# 請確保你的電腦已接上電源，並準備好讓 GPU 開始工作。\n",
    "# 你可以打開工作管理員來監控 GPU 使用率。\n",
    "\n",
    "# 建立 StratifiedKFold 物件\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# 準備 K-Fold 切分的資料\n",
    "X_kfold = train_val_df['filepath']\n",
    "y_kfold = train_val_df['label_idx']\n",
    "\n",
    "# 儲存每一折的最佳驗證準確率\n",
    "fold_val_accuracies = []\n",
    "\n",
    "# --- 主迴圈開始 ---\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(X_kfold, y_kfold)):\n",
    "    # ... (前面的程式碼不變) ...\n",
    "\n",
    "    # --- 1. 準備這一折的資料 ---\n",
    "    train_df = train_val_df.iloc[train_ids]\n",
    "    val_df = train_val_df.iloc[val_ids]\n",
    "\n",
    "    train_dataset = PlantDiseaseDataset(train_df, transform=data_transforms['train'])\n",
    "    val_dataset = PlantDiseaseDataset(val_df, transform=data_transforms['val'])\n",
    "\n",
    "    # *** 核心修正點：將 num_workers 改為 0 ***\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # --- 2. 建立全新的模型、損失函數和優化器 ---\n",
    "    model = BaselineModel(NUM_CLASSES).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # *** 修正點：移除 verbose=True 參數 ***\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3)\n",
    "\n",
    "    # --- 3. 訓練迴圈 (Epoch Loop) ---\n",
    "    num_epochs = 30\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # 訓練\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # 驗證\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # 更新學習率 (ReduceLROnPlateau 預設會在觸發時打印訊息)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 檢查是否需要早停或儲存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}. Saving model...\")\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f'baseline_model_fold_{fold+1}_best.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    fold_val_accuracies.append(best_val_acc)\n",
    "    end_time = time.time()\n",
    "    print(f\"Fold {fold+1} finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# --- 4. 評估 K-Fold 交叉驗證的最終結果 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"       CROSS-VALIDATION FINAL RESULTS       \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mean_accuracy = np.mean(fold_val_accuracies)\n",
    "std_accuracy = np.std(fold_val_accuracies)\n",
    "\n",
    "for i, acc in enumerate(fold_val_accuracies):\n",
    "    print(f\"Fold {i+1} Best Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"基準模型 5-折交叉驗證結果\")\n",
    "print(f\"平均驗證準確率: {mean_accuracy:.4f}\")\n",
    "print(f\"驗證準確率標準差: {std_accuracy:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantdisease_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
